{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Centric Approach.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvrqRxILaLl9"
      },
      "source": [
        "#Data Centric Approach\n",
        "In this notebook we will follow a data centric approach and try to improve predictions for relKa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ANR2JL-Zntf"
      },
      "source": [
        "#Mount  Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/My\\ Drive/Colab\\ Notebooks/ML\\ Project2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx22-cLKaEEq"
      },
      "source": [
        "#Usefull Libraries\n",
        "\n",
        "# pandas / numpy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#import xgboost regressor model\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "#pearson corelation coefficient\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "#import cross validation function from sklearn\n",
        "from sklearn.model_selection import cross_val_score\n",
        "#import one hot encoder function from sklearn\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "#import the train test split function from sklearn to split the dataset randomly for training and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import mse loss\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu0Qvdd8cQ8W"
      },
      "source": [
        "#import data from zip\n",
        "pddata = pd.read_csv('GSM1586785_ScrH-12A_Exd_14mer_cg.csv.zip', compression='zip', error_bad_lines=False)\n",
        "#print the first samples of the dataframe to get a look at the data\n",
        "pddata.head(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNpiW07vc0X1"
      },
      "source": [
        "#convert dataframe to np array\n",
        "data = pddata.to_numpy()\n",
        "#labels\n",
        "relKa = data[:,320:]\n",
        "# features\n",
        "# we remove the index of the data samples, since it is not a feature\n",
        "features = data[:,1:320]\n",
        "# temporarily remove the 2nd feature because one hot encoding causes ram overflow\n",
        "features = features[:,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgBLOflbtH8G"
      },
      "source": [
        "### Preprocessing of data\n",
        "\n",
        "One hot encoding the data is troubling for my setup and for google colab. The first one is unable to perform the task in reasonable time, while the second one runs out of RAM. I tried both the encoder os scikit learn and a custom encoder of mine that you can find here. The main problem is the size of the end result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_gmF27Iyj91"
      },
      "source": [
        "def one_hot_encoder(pos,length):\n",
        "  \"\"\"\n",
        "  One hot encode a list of unique elements\n",
        "\n",
        "  @param pos: int64 \n",
        "  @param length: int64 \n",
        "  \"\"\"\n",
        "  encoded = np.zeros(length)\n",
        "  encoded[pos] = 1\n",
        "  return encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S41_K7nfv13T"
      },
      "source": [
        "#remove duplicate sequences in case they exist\n",
        "#Kmer = [one_hot_encoder(i,len(relKa[:,0])) for i in range(len(relKa[:,0]))]\n",
        "#create the encoding for all sequences\n",
        "#Kmer_encode = [ for i in range(len(Kmer))]\n",
        "#Kmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J0xEIZstN1P"
      },
      "source": [
        "\n",
        "# initialize a one hot encoder that ignores the unseen sequences\n",
        "# enc = OneHotEncoder(handle_unknown='ignore')\n",
        "# use the sequences of the data for the initialization\n",
        "# enc.fit(Kmer_encode)\n",
        "# encode the sequences\n",
        "# enc.transform(Kmer_encode).toarray()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxct5Fcv4Eip"
      },
      "source": [
        "def standardization(x):\n",
        "  \"\"\"\n",
        "  Standardization of elements for a vector x\n",
        "  @param:x np.ndarray\n",
        "  \"\"\"\n",
        "  mean = np.mean(x)\n",
        "  #print(mean)\n",
        "  std = np.std(x)\n",
        "  #print(std)\n",
        "  x = np.apply_along_axis(lambda y: (y - mean)/std, 0, x)\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QGw-cM33ZDe"
      },
      "source": [
        "#standardize all features\n",
        "std_features=np.apply_along_axis(standardization, 1, features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTSgJrs9SKpI"
      },
      "source": [
        "# randomly split the dataset in a 70/30 split\n",
        "# change the train_size for partitions of different size\n",
        "# rerun for different partitions\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, relKa,  train_size=0.7, random_state=33, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIo35hDWiDe0"
      },
      "source": [
        "## Initial Training of the regression method \n",
        "\n",
        "We choose Xgboost as our model for this approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU16e-mhiBEx"
      },
      "source": [
        "#Cross validation for initial data and Xgboost\n",
        "scores = cross_val_score(XGBRegressor(objective='reg:squarederror'), features, relKa, scoring='neg_mean_squared_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ2LZdh5rKu5"
      },
      "source": [
        "print(\"Basic loss for XGBoost:\",np.mean((-scores)**0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUJSmXAVm8kD"
      },
      "source": [
        "#Set the parameters for the XGBRegressor\n",
        "#we will use the GPU, otherwise it does not train\n",
        "param_dict = {\n",
        "    'max_depth':10,\n",
        "    'n_estimators':1000,\n",
        "    'objective': 'reg:squarederror',\n",
        "    'tree_method': 'gpu_hist'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R7Xd2PwRFp8"
      },
      "source": [
        "#model of XBGRegressor without extensive parameter tuning\n",
        "model=XGBRegressor(**param_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxuOs2e6RX-d"
      },
      "source": [
        "#fit the model\n",
        "model.fit(X_train,y_train,eval_metric=\"rmse\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxN0QhOtUROH"
      },
      "source": [
        "# do some predictions with the model\n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cbsSZUZ7xhf"
      },
      "source": [
        "#mean squared error\n",
        "mean_squared_error(y_test,y_pred)\n",
        "#np.corrcoef(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}