{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "qeM593-TwcoM"
      },
      "outputs": [],
      "source": [
        "# for dealing with data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#for visualization\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as st\n",
        "import random\n",
        "\n",
        "\n",
        "#machine learning libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import PredefinedSplit\n",
        "\n",
        "#for mathematics and statistics\n",
        "import math\n",
        "from scipy import stats\n",
        "import scipy as sci\n",
        "from scipy.spatial.distance import minkowski\n",
        "from scipy.stats import spearmanr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DD9gASEwigQ",
        "outputId": "fab1671e-b96c-410f-df97-b809a53c960a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'drive/My Drive'\n",
            "/content/drive/My Drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/My\\ Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "wcIPdcE4wlb1"
      },
      "outputs": [],
      "source": [
        "# we import the dataset using pandas\n",
        "pddata = pd.read_csv('GSM1586785_ScrH-12A_Exd_14mer_cg.csv.zip', compression='zip', error_bad_lines=False, skiprows=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "FU9GUWIBwnDh"
      },
      "outputs": [],
      "source": [
        "# we visualize the first ten rows of our dataframe\n",
        "pddata = pddata.to_numpy()\n",
        "RelKa = pddata[:, -1]\n",
        "training_data = pddata[:, 2:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "ec7aw0cE6rsV"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "def Anomaly_Detection_Isolation_Forests(x, change_split=True):\n",
        "  random_state = np.random.RandomState(42)\n",
        "  contamination = 'auto'\n",
        "  threshold = np.random.uniform(-0.03, -0.02, 1)\n",
        "  model = IsolationForest(n_estimators=120, max_samples='auto', contamination=contamination, random_state=random_state)\n",
        "  model.fit(x)\n",
        "  scores = model.decision_function(x)\n",
        "  if change_split == False:\n",
        "    anomaly_score = model.predict(x)\n",
        "    outliers_indices = np.where(anomaly_score == -1)[0]\n",
        "  if change_split == True:\n",
        "    outliers_indices = split_outliers(threshold, scores)\n",
        "  return contamination, scores, outliers_indices\n",
        "\n",
        "def check_Isolation_Forests(contamination, outliers_indices):\n",
        "  \"\"\"\n",
        "  Simply a check on the proper working of the IF algorithm\n",
        "  \"\"\"\n",
        "  tol = 1.0e-02\n",
        "  if contamination != 'auto':\n",
        "    outliers_percentage = 1 / len(RelKa) * len(outliers_indices)\n",
        "    assert np.abs(contamination-outliers_percentage) < tol\n",
        "\n",
        "def check_boundary_decision(scores, p, verbose=1):\n",
        "  \"\"\"\n",
        "  This function simply controls how many scores returned by the IF algorithm \n",
        "  are likely to be misclassified\n",
        "  \"\"\"\n",
        "  indecision_percentage = 1 / len(RelKa) * np.count_nonzero(np.abs(scores) <= p)\n",
        "  if verbose == 1:\n",
        "    plt.hist(scores)\n",
        "    plt.show()\n",
        "    print(\"The indecision percentage around\", p,  \"is\", indecision_percentage)\n",
        "    print(\"The percentage of outliers detected is\", 1 / len(scores) * len(np.where(scores < 0)[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "d8eYszg3n53z"
      },
      "outputs": [],
      "source": [
        "def split_importance(x, y, importance_class=0.7):\n",
        "  \"\"\"\n",
        "  Split the samples into interesting ones and not interesting ones\n",
        "  :param x: numpy.ndarray:the feature vector of the initial dataset\n",
        "  :param y: numpy.ndarray: the value vector of the initial dataset\n",
        "  :param importance_class: float: the lower bound for the underepresented class\n",
        "  \"\"\"\n",
        "  return x[y >= importance_class], y[y >= importance_class], x[y < importance_class], y[y < importance_class]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "UuXU1lLfoAy1"
      },
      "outputs": [],
      "source": [
        "x_1, y_1, x_0, y_0 = split_importance(training_data, RelKa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "wnZYvVZuoEXN"
      },
      "outputs": [],
      "source": [
        "# split the two classes seperately into train and test set to ensure representation of the minority class\n",
        "# both in the test set and in the train set\n",
        "x_1_train, x_1_test, y_1_train, y_1_test = train_test_split(x_1, y_1, test_size=0.3, random_state=42)\n",
        "x_0_train, x_0_test, y_0_train, y_0_test = train_test_split(x_0, y_0, test_size=0.3, random_state=42)\n",
        "\n",
        "x_train = np.concatenate((x_1_train, x_0_train))\n",
        "y_train = np.concatenate((y_1_train, y_0_train))\n",
        "x_test = np.concatenate((x_1_test, x_0_test))\n",
        "y_test = np.concatenate((y_1_test, y_0_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "GUMWzdqPkXHu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "4a213077-d02f-49ce-b9a6-13d60e4ff639"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-3c2329bc20e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutliers_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnomaly_Detection_Isolation_Forests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutliers_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-118-a8731eddc683>\u001b[0m in \u001b[0;36mAnomaly_Detection_Isolation_Forests\u001b[0;34m(x, change_split)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIsolationForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontamination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontamination\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mchange_split\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_iforest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \"\"\"\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;31m# Pre-sort indices to avoid that each individual tree of the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    736\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "_, _, outliers_indices = Anomaly_Detection_Isolation_Forests(x_train)\n",
        "x_train, y_train = drop_outliers(x_train, y_train, outliers_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "vO7-bhBsoMWO"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "4fsfcd-qF3YL"
      },
      "outputs": [],
      "source": [
        "def random_undersampling(x, y, alpha, importance_boundary=0.7):\n",
        "  \"\"\"\n",
        "  alpha is a parameter bigger than 1/2\n",
        "  \"\"\"\n",
        "  assert alpha > 0.5\n",
        "  count = 0\n",
        "  to_drop = []\n",
        "  for i in range(x.shape[0]):\n",
        "    print(\"Checking the \", i, \"sample\")\n",
        "    U = st.uniform.rvs(size=1)\n",
        "    if y[i] <= importance_boundary:\n",
        "      if U <= alpha:\n",
        "        to_drop.append(i)\n",
        "  x = np.delete(x, to_drop, axis=0)\n",
        "  y = np.delete(y, to_drop, axis=0)\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "JBdxcA968_G4"
      },
      "outputs": [],
      "source": [
        "def stratified_undersampling(x, y, alpha=1, intervals = 0.05 * np.arange(0, 15)):\n",
        "  \"\"\"\n",
        "  This function performs stratified undersampling on the majority class\n",
        "  by dropping samples in each strata with a probability equal to the frequency\n",
        "  :param x: the datapoints to be undersampled\n",
        "  :param y: the corresponding labels\n",
        "  :param alpha: a hyperparameter to choose the dropping probability \n",
        "  :param intervals: the stratification intervals\n",
        "  :return x and y undersampled  \n",
        "  \"\"\"\n",
        "  # just a check that we drop with high probability in the frequent strata\n",
        "  assert alpha > 0.5\n",
        "  to_drop = []\n",
        "  # we do a cycle over all intervals\n",
        "  for i in range(len(intervals)-1):\n",
        "    # get the indices for which the label is in the i-th strata\n",
        "    indices = np.where(np.logical_and(intervals[i] < y, y < intervals[i+1]))[0]\n",
        "    # compute the frequency of the i-th strata\n",
        "    frequency = len(indices) / len(y)\n",
        "    # we do a cycle over all elements in such a stratum\n",
        "    for j in range(len(indices)):\n",
        "      # we draw a uniform random variable\n",
        "      U = st.uniform.rvs(size=1)\n",
        "      if U <= alpha * frequency:\n",
        "        # we drop every sample in the i-th strata with probability alpha * frequency\n",
        "        to_drop.append(indices[j])\n",
        "  # we drop from x and y the samples and the labels found before\n",
        "  x_new = np.delete(x, to_drop, axis=0)\n",
        "  y_new = np.delete(y, to_drop, axis=0)\n",
        "  return x_new, y_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "4As9ODx8t6_H"
      },
      "outputs": [],
      "source": [
        "#quantization function required for some score functions\n",
        "def quantize(x,cuts=100):\n",
        "  ranges = np.sort(np.unique(pd.cut(x,cuts)))\n",
        "  for i in range(ranges.shape[0]):\n",
        "    ranges_i = ranges[i]\n",
        "    x[(x<=ranges_i.right)&(x>ranges_i.left)] = (ranges_i.left +ranges_i.right)/2\n",
        "  return x\n",
        "\n",
        "def quantize_features(x,cuts=100):\n",
        "  return np.apply_along_axis(quantize,0,x,cuts)\n",
        "\n",
        "#correlation scorers\n",
        "def Spearman(i,j):\n",
        "  ret,_ = spearmanr(i,j)\n",
        "  return ret\n",
        "\n",
        "def Correlation_Score(x,y):\n",
        "  x = x.T\n",
        "  score = [min([abs(Spearman(j,i)) for i in x]) for j in x]\n",
        "  return np.array(score)\n",
        "\n",
        "def Correlation_Score_Max(x,y):\n",
        "  x = x.T\n",
        "  score = [-max([abs(Spearman(j,i)) for i in x]) for j in x]\n",
        "  return np.array(score)\n",
        "\n",
        "def Closest(x, z):\n",
        "    \"\"\"\n",
        "    Returns the index of the sample in x which is the closest one to all the samples in z\n",
        "    :param x: numpy.ndarray: The array of the samples for which we seek to find the furthest\n",
        "    :param z: numpy.ndarray: The array of the samples which we use as a point of reference\n",
        "    :return: int: returns a single index corresponding to the furthest x\n",
        "    \"\"\"\n",
        "    distances = []\n",
        "    # we seek for the closest point to z in x\n",
        "    for i in range(x.shape[0]):\n",
        "        # for each sample in x we calculate the distance from all points in z and we take the minimum\n",
        "        distance = min([np.linalg.norm(j - x[i]) for j in z])\n",
        "        # we add the calculated distance to the list\n",
        "        distances.append(distance)\n",
        "    # we return the closest point by taking the argmin over all calculated distances\n",
        "    return np.argmin(distances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "JtmKdxD5xcMm"
      },
      "outputs": [],
      "source": [
        "def PSU_undersampling_regression(x, y, randomsize, xi, yi):\n",
        "    \"\"\"\n",
        "    This function performs the PSU undersampling of the samples in the x,y arrays\n",
        "    with regards to the set xi,yi. This version is a slight modification of the original proposed algorithm\n",
        "    which helps us improving in the prediction of the RelKa in the central intervals\n",
        "    :param x: numpy.ndarray: The array of samples on which we perform the undersampling\n",
        "    :param y: numpy.ndarray: The labels corresponding to the samples in x\n",
        "    :param randomsize: int: An integer corresponding to the number of samples to be left\n",
        "    :param xi: numpy.ndarray: The array with the samples to be used as a reference for distances\n",
        "    :param yi: numpy.ndarray: The labels of the samples in xi\n",
        "    :return: numpy.ndarray,numpy.ndarray: Returns 2 numpy arrays corresponding to the undersampled data\n",
        "    \"\"\"\n",
        "    # we calculate the centroid of the majority class data\n",
        "    C = np.mean(x, axis=0)\n",
        "    # for each sample in the majority class we calculate the l2 distance from the centroid\n",
        "    dist = np.linalg.norm(x - C, 2, axis=1)\n",
        "    # we sort the distances\n",
        "    indices = dist.argsort()\n",
        "    # we sort the samples and the corresponding labels according to the distances calculated before\n",
        "    x = x[indices]\n",
        "    y = y[indices]\n",
        "    # we split the samples and the corresponding labels in randomsize partitions\n",
        "    split_x = np.array_split(x, randomsize)\n",
        "    split_y = np.array_split(y, randomsize)\n",
        "    # for each partition we calculate the closest point to the minority class xi\n",
        "    indices = [Closest(split_x[i], xi) for i in range(randomsize)]\n",
        "    # the collection of the randomsize closest points calculated above represents the undersampled data\n",
        "    x_resample = np.array([split_x[i][indices[i]] for i in range(randomsize)])\n",
        "    # the corresponding labels of the undersampled data are also returned\n",
        "    y_resample = np.array([split_y[i][indices[i]] for i in range(randomsize)])\n",
        "    return x_resample, y_resample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "FcGYPv2Cvwoe"
      },
      "outputs": [],
      "source": [
        "def PSU_stratified_undersampling(x, y, randomsize, intervals, xi, yi):\n",
        "    \"\"\"\n",
        "    This function performs the PSU undersampling of the samples in the x,y arrays\n",
        "    with regards to the set xi,yi. This version is a slight modification of the original proposed algorithm\n",
        "    which helps us improving in the prediction of the RelKa in the central intervals\n",
        "    :param x: numpy.ndarray: The array of samples on which we perform the undersampling\n",
        "    :param y: numpy.ndarray: The labels corresponding to the samples in x\n",
        "    :param randomsize: int: An integer corresponding to the number of samples to be left\n",
        "    :param intervals: the intervals for stratification\n",
        "    :param xi: the reference points\n",
        "    :param yi: the corresponding labels to xi\n",
        "    :return: numpy.ndarray,numpy.ndarray: Returns 2 numpy arrays corresponding to the undersampled data\n",
        "    \"\"\"\n",
        "    # we calculate the number of strata\n",
        "    k = intervals.shape[0]\n",
        "    print(k)\n",
        "    # we initialize the lists for undersampling\n",
        "    x_resample = np.zeros(x.shape[1])\n",
        "    x_resample = x_resample[..., np.newaxis].T\n",
        "    y_resample = []\n",
        "    for i in range(len(intervals)-1):\n",
        "        indices = np.where(np.logical_and(intervals[i] < y, y < intervals[i+1]))[0]\n",
        "        # we get the samples and the labels in the i-th strata\n",
        "        x_ = x[indices]\n",
        "        y_ = y[indices]\n",
        "        frequency = len(indices) / len(y)\n",
        "        # we perform the modified PSU undersampling on the i-th strata\n",
        "        x_, y_ = PSU_undersampling_regression(x_, y_, int(randomsize*frequency), xi, yi)\n",
        "        # we add the samples and the corresponding labels\n",
        "        x_resample = np.concatenate([x_resample, x_], axis=0)\n",
        "        y_resample = np.concatenate([y_resample, y_], axis=0)\n",
        "    # we drop the first row used only for initialization\n",
        "    x_resample = np.delete(x_resample, 0, axis=0)\n",
        "    return x_resample, y_resample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "t5MjEsO7t-y-"
      },
      "outputs": [],
      "source": [
        "def Furthest(x, z):\n",
        "    \"\"\"\n",
        "    Returns the index of the sample in x which is furthest from all the samples in z\n",
        "    :param x: numpy.ndarray: The array of the samples for which we seek to find the furthest\n",
        "    :param z: numpy.ndarray: The array of the samples which we use as a point of reference\n",
        "    :return: int: returns a single index corresponding to the furthest x\n",
        "    \"\"\"\n",
        "    # we initialize the maximum distance to be -1 and the furthest sample to have index 0\n",
        "    max_dist_i = 0\n",
        "    max_dist = -1\n",
        "    # we seek for the furthest point from z in x\n",
        "    for i in range(x.shape[0]):\n",
        "        # for each sample in x we calculate the distance from all points in z and we take the minimum\n",
        "        distance = min([np.linalg.norm(j - x[i]) for j in z])\n",
        "        # we update the distance if the current distance is larger than the old one\n",
        "        if max_dist < distance:\n",
        "            max_dist = distance\n",
        "            # we save the current sample i as the candidate furthest point from z\n",
        "            max_dist_i = i\n",
        "    return max_dist_i\n",
        "\n",
        "def PSU_undersampling(x,y,randomsize,xi,yi):\n",
        "  \"\"\"\n",
        "  This function calculates the PSU undersampling of the samples in the x,y arrays\n",
        "  with regards to the set xi,yi\n",
        "  :param x: numpy.ndarray: The array of samples, which we want to undersample\n",
        "  :param y: numpy.ndarray: The array with the labels corresponding to the samples in x\n",
        "  :param randomsize: int: An int corresponding to the number of samples to be left\n",
        "  :param xi: numpy.ndarray: The array with the samples to be used as a reference for distances\n",
        "  :param yi: numpy.ndarray: The labels of these samples\n",
        "  :return: numpy.ndarray,numpy.ndarray: Returns 2 numpy arrays corresponding to the undersampled data\n",
        "  \"\"\"\n",
        "  C = np.mean(x,axis = 0)\n",
        "  dist = np.linalg.norm(x-C,2,axis = 1)\n",
        "  indices = dist.argsort()\n",
        "  x = x[indices]\n",
        "  y = y[indices]\n",
        "  split_x = np.array_split(x,randomsize)\n",
        "  split_y = np.array_split(y,randomsize)\n",
        "  indices = [Furthest(split_x[i],xi) for i in range(randomsize)]\n",
        "  x_resample = np.array([split_x[i][indices[i]] for i in range(randomsize)])\n",
        "  y_resample = np.array([split_y[i][indices[i]] for i in range(randomsize)])\n",
        "  return x_resample, y_resample\n",
        "\n",
        "def PSU_undersampling_reduced_dim(x,y,randomsize,xi,yi):\n",
        "  \"\"\"\n",
        "  This function calculates the PSU undersampling by first doing a dimensionality reduction\n",
        "  of the samples in the x,y arrays with regards to the set xi,yi\n",
        "  :param x: numpy.ndarray: The array of samples, which we want to undersample\n",
        "  :param y: numpy.ndarray: The array with the labels corresponding to the samples in x\n",
        "  :param randomsize: int: An int corresponding to the number of samples to be left\n",
        "  :param xi: numpy.ndarray: The array with the samples to be used as a reference for distances\n",
        "  :param yi: numpy.ndarray: The labels of these samples\n",
        "  :return: numpy.ndarray,numpy.ndarray: Returns 2 numpy arrays corresponding to the undersampled data\n",
        "  \"\"\"\n",
        "  feature_scores =  Fisher_Score(xi,x)\n",
        "  indices = np.sort((-feature_scores).argsort()[:10])\n",
        "  x_filtered = x[:,indices]\n",
        "  x_i = xi[:,indices]\n",
        "  C = np.mean(x_filtered,axis = 0)\n",
        "  dist = np.linalg.norm(x_filtered-C,2,axis = 1)\n",
        "  indices = dist.argsort()\n",
        "  x_filtered = x_filtered[indices]\n",
        "  y = y[indices]\n",
        "  split_x = np.array_split(x_filtered,randomsize)\n",
        "  split_y = np.array_split(y,randomsize)\n",
        "  indices = [Furthest(split_x[i],x_i) for i in range(randomsize)]\n",
        "  split_x = np.array_split(x,randomsize)\n",
        "  x_resample = np.array([split_x[i][indices[i]] for i in range(randomsize)])\n",
        "  y_resample = np.array([split_y[i][indices[i]] for i in range(randomsize)])\n",
        "  return x_resample, y_resample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "uJq6YmmPoTM1"
      },
      "outputs": [],
      "source": [
        "def Fisher_Score(x_import, x_nimport):\n",
        "  \"\"\"\n",
        "  Given two arrays of two classes this function calculates the Fischer_scores to \n",
        "  measure the significance for all features\n",
        "  :param x_import: numpy.ndarray: the array containing the samples of one class\n",
        "  :param x_nimport: numpy.ndarray: the array containing the samples of the other class\n",
        "  :return: numpy.ndarray: returns an array containg the Fisher_Score for all features \n",
        "  \"\"\"\n",
        "  mean_import = np.mean(x_import, axis = 0)\n",
        "  mean_nimport = np.mean(x_nimport, axis = 0)\n",
        "  mean_dist = np.absolute(mean_import - mean_nimport)\n",
        "  std_import = np.std(x_import, axis=0)\n",
        "  std_nimport = np.std(x_nimport, axis=0)\n",
        "  std_sum = std_import+std_nimport\n",
        "  #return std_sum\n",
        "  return np.divide(mean_dist,std_sum)\n",
        "\n",
        "def calculate_distances(x, distance):\n",
        "  \"\"\"\n",
        "  Calculates the distance between any two pairs of the set x using \n",
        "  the Minkowski distance of degree distance.\n",
        "  :param x: numpy.ndarray: the vector for which we will calculate the distance \n",
        "                            between all of its elements \n",
        "  :param distance: float: the norm which should be used for the Minkowski distance\n",
        "  :return: numpy.ndarray: returns the Minkowski distance with the specified norm \n",
        "                          between all pairs of elements in x \n",
        "  \"\"\"\n",
        "  dist = np.array([[minkowski(a1,a2,distance) for a2 in x] for a1 in x])\n",
        "  np.fill_diagonal(dist,float('inf'))\n",
        "  return dist\n",
        "\n",
        "def random_sampler(x, y, randomsize):\n",
        "  \"\"\"\n",
        "  This function does random undersampling of vectors x,y and reduces them to \n",
        "  size random size\n",
        "  :param x: numpy.ndarray: the feature vector to be subsampled\n",
        "  :param y: numpy.ndarray: the label vector to be subsampled\n",
        "  :return: <class 'tuple'>: A tuple containing the two undersampled vectors\n",
        "  \"\"\"\n",
        "  p = np.random.permutation(len(y))\n",
        "  new_x = x[p]\n",
        "  new_y = y[p]\n",
        "  return new_x[:randomsize],new_y[:randomsize]\n",
        "\n",
        "def generate_samples(x, y, neighbors, N):\n",
        "  \"\"\"\n",
        "  This function generate N samples which are convex combinations of \n",
        "  the features of x and the labels of y\n",
        "  :param x: numpy.ndarray:\n",
        "  :param y: numpy.ndarray:\n",
        "  :return: <class 'tuple'>:\n",
        "  \"\"\"\n",
        "  new_samples_x = []\n",
        "  new_samples_y = []\n",
        "  for i in range(N):\n",
        "    random_sample_i = random.randint(0,y.shape[0]-1)\n",
        "    x_i = x[random_sample_i]\n",
        "    random_sample_j = random.randint(0,neighbors.shape[1]-1)\n",
        "    neigh_i = neighbors[random_sample_i,random_sample_j]\n",
        "    x_j = x[neigh_i]\n",
        "    lambda_ = random.uniform(0,1)\n",
        "    y_i = y[random_sample_i]\n",
        "    y_j = y[neigh_i]\n",
        "    new_x = x_i + lambda_*(x_j-x_i)\n",
        "    new_y = y_i + lambda_*(y_j-y_i)\n",
        "    new_samples_x.append(new_x)\n",
        "    new_samples_y.append(new_y)\n",
        "  return np.array(new_samples_x),np.array(new_samples_y)\n",
        "\n",
        "def random_sampler(x,y,randomsize):\n",
        "  \"\"\"\n",
        "  This function does random undersampling of vectors x,y and reduces them to \n",
        "  size random size\n",
        "  :param x: numpy.ndarray: the feature vector to be subsampled\n",
        "  :param y: numpy.ndarray: the label vector to be subsampled\n",
        "  :return: <class 'tuple'>: A tuple containing the two undersampled vectors\n",
        "  \"\"\"\n",
        "  p = np.random.permutation(len(y))\n",
        "  new_x = x[p]\n",
        "  new_y = y[p]\n",
        "  return new_x[:randomsize],new_y[:randomsize]\n",
        "\n",
        "def generate_samples(x,y,neighbors,N):\n",
        "  \"\"\"\n",
        "  This function generate N samples which are convex combinations of \n",
        "  the features of x and the labels of y\n",
        "  :param x: numpy.ndarray:\n",
        "  :param y: numpy.ndarray:\n",
        "  :return: <class 'tuple'>:\n",
        "  \"\"\"\n",
        "  new_samples_x = []\n",
        "  new_samples_y = []\n",
        "  for i in range(N):\n",
        "    random_sample_i = random.randint(0,y.shape[0]-1)\n",
        "    x_i = x[random_sample_i]\n",
        "    random_sample_j = random.randint(0,neighbors.shape[1]-1)\n",
        "    neigh_i = neighbors[random_sample_i,random_sample_j]\n",
        "    x_j = x[neigh_i]\n",
        "    lambda_ = random.uniform(0,1)\n",
        "    y_i = y[random_sample_i]\n",
        "    y_j = y[neigh_i]\n",
        "    new_x = x_i + lambda_*(x_j-x_i)\n",
        "    new_y = y_i + lambda_*(y_j-y_i)\n",
        "    new_samples_x.append(new_x)\n",
        "    new_samples_y.append(new_y)\n",
        "  return np.array(new_samples_x),np.array(new_samples_y)\n",
        "\n",
        "def smote_sf_classification(x, y, undersample=0.1, oversample=0.3, attribute_scorer=Fisher_Score,\n",
        "                            attribute_number=10, distance=float('inf'), kneighbors=3, importance_class=0.7):\n",
        "    \"\"\"\n",
        "    This function takes the complete input and produces a more balanced dataset based on the importance class\n",
        "    :param x: numpy.ndarray: the feature vector of the initial dataset\n",
        "    :param y: numpy.ndarray: the value vector of the initial dataset\n",
        "    :param undersample: float: the percentage of the dominant class that we want to keep\n",
        "    :param oversample: float: the percentage of the dataset that the minority class will be at the end\n",
        "    :param attribute_scorer: function: a function which will be used to score the relevance of a feature\n",
        "    :param attribute_number: int: the number of attributes to keep according to their score\n",
        "    :param distance: float: the norm which should be used for the Minkowski distance\n",
        "    :param kneighbors: int: the number of nearest neighbours to be considered for each point\n",
        "    :param importance_class: float: the lower bound for the under-represented class\n",
        "    :return: returns 2 new feature vectors and 2 new label vectors containing\n",
        "            the data for the importance class and the data for the non importance\n",
        "            class and their labels.\n",
        "    \"\"\"\n",
        "    # we split the training set into majority and minority class according to the value of the label\n",
        "    x_import = x[y >= importance_class]\n",
        "    y_import = y[y >= importance_class]\n",
        "    x_nimport = x[y < importance_class]\n",
        "    y_nimport = y[y < importance_class]\n",
        "\n",
        "    # we calculate the Fisher score for all the features\n",
        "    feature_scores = attribute_scorer(x_import, x_nimport)\n",
        "    # we find the attribute_number highest coordinates of the feature_scores vector\n",
        "    indices = np.sort((-feature_scores).argsort()[:attribute_number])\n",
        "    # we filter the samples to be represented only by the attribute number highest scored features\n",
        "    x_import_filtered = x_import[:, indices]\n",
        "    # we calculate the distances between the datapoints considering only the important features as otherwise we\n",
        "    # fall into the curse of dimensionality\n",
        "    distances = calculate_distances(x_import_filtered, distance)\n",
        "    # we find the kneighbors lowest indices of the distances of each sample, the corresponding datapoints are the\n",
        "    # nearest neighbours to be drawn randomly for the oversampling procedure\n",
        "    neighbors = np.array([np.sort(d.argsort()[:kneighbors]) for d in distances])\n",
        "    # we first undersample the the majority class\n",
        "    # we calculate the final size of the undersampled data\n",
        "    nimport_len = int(undersample * y_nimport.shape[0])\n",
        "    # we perform undersampling\n",
        "    x_nimport, y_nimport = PSU_undersampling(x_nimport, y_nimport, nimport_len, x_import, y_import)\n",
        "    # we calculate the number of samples to be generated by the oversampling algorithm\n",
        "    N = int(oversample * (y_nimport.shape[0]) - y_import.shape[0])\n",
        "    # we perform oversampling\n",
        "    new_samples_x, new_samples_y = generate_samples(x_import, y_import, neighbors, N)\n",
        "    # we merge the new samples with the old samples of the minority class and their labels\n",
        "    x_import = np.concatenate((x_import, new_samples_x))\n",
        "    y_import = np.concatenate((y_import, new_samples_y))\n",
        "    # we stack the samples and the labels of the majority and minority class\n",
        "    x_ret = np.concatenate((x_import, x_nimport))\n",
        "    y_ret = np.concatenate((y_import, y_nimport))\n",
        "    # x_ret represents the balanced dataset and y_ret the corresponding labels\n",
        "    return x_ret, y_ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "RzW1M9AHv8Dk"
      },
      "outputs": [],
      "source": [
        "def smote_sf_regression(x, y, undersample=0.1, oversample=0.3, attribute_scorer=Fisher_Score,\n",
        "                        attribute_number=10, distance=float('inf'), kneighbors=3,\n",
        "                        importance_class=0.7, width_strata=0.1):\n",
        "    \"\"\"\n",
        "    This function performs stratified undersampling and SMOTE oversampling for the regression task\n",
        "    @param x: the initial dataset\n",
        "    @param y: the corresponding labels\n",
        "    @param undersample: the undersampling rate\n",
        "    @param oversample: the oversampling rate\n",
        "    @param attribute_scorer: the scorer for the features\n",
        "    @param attribute_number: the number of features to be considered for the distances between samples\n",
        "    @param distance: the degree of the Minkowski distance to be used\n",
        "    @param kneighbors: the number of nearest neighbours for the oversampling algorithm\n",
        "    @param importance_class: the importance boundary between majority and minority class\n",
        "    @param width_strata: the width of the strata for the stratified undersampling\n",
        "    @return: x and y rebalanced\n",
        "    \"\"\"\n",
        "    # we split the training set into majority and minority class according to the value of the label\n",
        "    x_import = x[y >= importance_class]\n",
        "    y_import = y[y >= importance_class]\n",
        "    x_nimport = x[y < importance_class]\n",
        "    y_nimport = y[y < importance_class]\n",
        "\n",
        "    # we calculate the Fisher score for all the features\n",
        "    feature_scores = attribute_scorer(x_import, x_nimport)\n",
        "    # we find the attribute_number highest coordinates of the feature_scores vector\n",
        "    indices = np.sort((-feature_scores).argsort()[:attribute_number])\n",
        "    # we filter the samples to be represented only by the attribute number highest scored features\n",
        "    x_import_filtered = x_import[:, indices]\n",
        "    # we calculate the distances between the datapoints considering only the important features as otherwise we\n",
        "    # fall into the curse of dimensionality\n",
        "    distances = calculate_distances(x_import_filtered, distance)\n",
        "    # we find the kneighbors lowest indices of the distances of each sample, the corresponding datapoints are the\n",
        "    # nearest neighbours to be drawn randomly for the oversampling procedure\n",
        "    neighbors = np.array([np.sort(d.argsort()[:kneighbors]) for d in distances])\n",
        "    # we get the undersampled datasize\n",
        "    nimport_len = int(undersample * y_nimport.shape[0])\n",
        "    print(nimport_len)\n",
        "    # we perform stratified undersampling\n",
        "    intervals = width_strata * np.arange(int(importance_class / width_strata))\n",
        "    print(intervals)\n",
        "    x_nimport, y_nimport = stratified_undersampling(x_nimport, y_nimport, intervals)\n",
        "    # we compute the number of new samples to be generated\n",
        "    N = int(oversample * (y_nimport.shape[0]) - y_import.shape[0])\n",
        "    new_samples_x, new_samples_y = generate_samples(x_import, y_import, neighbors, N)\n",
        "    # we merge the new samples with the old samples of the minority class and their labels\n",
        "    x_import = np.concatenate((x_import, new_samples_x))\n",
        "    y_import = np.concatenate((y_import, new_samples_y))\n",
        "    # we stack the samples and the labels of the majority and minority class\n",
        "    x_ret = np.concatenate((x_import, x_nimport))\n",
        "    y_ret = np.concatenate((y_import, y_nimport))\n",
        "    # x_ret represents the balanced dataset and y_ret the corresponding labels\n",
        "    return x_ret, y_ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c4WT7AvpFRs",
        "outputId": "713b31f4-ac04-4a57-f44a-cb707269a03e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(88140, 318) (88140,)\n",
            "(8587, 318) (8587,)\n"
          ]
        }
      ],
      "source": [
        "x_train, y_train = smote_sf_regression(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "ESd8q6QBxvdu",
        "outputId": "8c684161-637a-4920-8ef1-6ba292d65580"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7efd71b28250>"
            ]
          },
          "metadata": {},
          "execution_count": 193
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 842.4x595.44 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.kdeplot(y_train, fill=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "a63d2612",
        "outputId": "2f43574e-df2d-49eb-e063-34ce3f0e670f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15:56:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-194-badf1672b5f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m model = XGBRegressor(subsample = 0.8999999999999999, n_estimators= 500,\n\u001b[1;32m      4\u001b[0m                      max_depth = 20, learning_rate = 0.01, colsample_bytree = 0.7999999999999999, colsample_bylevel = 0.6, obj=MSE)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    394\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# we use XGB regressor model with the following hyperparameters\n",
        "\n",
        "model = XGBRegressor(subsample = 0.8999999999999999, n_estimators= 500,\n",
        "                     max_depth = 20, learning_rate = 0.01, colsample_bytree = 0.7999999999999999, colsample_bylevel = 0.6, obj=MSE)\n",
        "model.fit(x_train, y_train,eval_metric=MSE)\n",
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6898c6e"
      },
      "outputs": [],
      "source": [
        "range_values = np.arange(0.05,1.05,0.1)\n",
        "mse_ranges = []\n",
        "for val in range_values:\n",
        "    labels_range = y_test[np.where(np.abs(y_test-val)<=0.05)]\n",
        "    correspondent_predictions = predictions[np.where(np.abs(y_test-val)<=0.05)]\n",
        "    mse_ranges.append(MSE(correspondent_predictions, labels_range))\n",
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "intervals = ['0.0-0.1', '0.1-0.2', '0.2-0.3', '0.3-0.4','0.4-0.5', '0.5-0.6', '0.6-0-7', '0.7-0.8', '0.8-0.9', '0.9-1.0']\n",
        "dict_mse = {'intervals': intervals, 'mse': mse_ranges}\n",
        "df_mse = pd.DataFrame.from_dict(dict_mse)\n",
        "sns.barplot(x = df_mse['intervals'], y = df_mse['mse'], color='blue')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}